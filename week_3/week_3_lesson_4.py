# -*- coding: utf-8 -*-
"""week_3_lesson_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RNN7ybJapA3lX4GZneyPEi40EOO2gSds

# Setting Up the Knowledge Base
"""

!pip install chromadb
!pip install langchain_huggingface

import chromadb
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings

# Initialize Chromadb
client = chromadb.PersistentClient(path="./research_db")
collection = client.get_or_create_collection(
    name="ml_publications",
    metadata={"hnsw:space": "cosine"}
)

# Set-up our embedding models
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

"""# Loading the Publication"""

!pip install langchain-community

import os
from langchain_community.document_loaders import TextLoader

def load_research_publications(documents_path):
  """Load research publications from .txt files and return as list of strings"""

  # List to store all documents
  documents = []

  # Load each .txt file in the documents folder
  for file in os.listdir(documents_path):
    if file.endswith(".txt"):
      file_path = os.path.join(documents_path, file)
      try:
        loader = TextLoader(file_path)
        loaded_docs = loader.load()
        documents.extend(loaded_docs)
        print(f"Successfully loaded: {file}")
      except Exception as e:
        print(f"Error loading file {file}: {str(e)}")  # Print the file and it's error message

  print(f"\nTotal documents loaded: {len(documents)}")

  # Extract content as strings and return
  publications = []
  for doc in documents:
    publications.append(doc.page_content)

  return publications

"""# Chunking the Publications"""

def chunk_research_paper(paper_content, title):
  """Break a research paper into searchable chunks"""

  text_splitter = RecursiveCharacterTextSplitter(
      chunk_size=1000,   # ~200 word per chunk
      chunk_overlap=200,   # overlap to preserve contex
      separators=["\n\n", "\n", ".", " ", ""]
  )

  chunks = text_splitter.split_text(paper_content)

  # Add metadata to each chunk
  chunk_data = []
  for i, chunk in enumerate(chunks):
    chunk_data.append({
        "content": chunk,
        "title": title,
        "chunk_id": f"{title}_{i}"
    })

  return chunk_data

"""# Create Embeddings"""

import torch
from langchain_huggingface import HuggingFaceEmbeddings

def embed_documents(documents: list[str]) -> list[list[float]]:
  """Embed documents using a model"""
  device = (
      "cuda"
      if torch.cuda.is_available()
      else "mps" if torch.backends.mps.is_available() else "cpu"
  )
  model = HuggingFaceEmbeddings(
      model_name="sentence-transformers/all-MiniLM-L6-v2",
      model_kwargs={"device": device },
  )

  embeddings = model.embed_documents(documents)
  return embeddings

"""# Storing In Vector Database"""

from chromadb.api.models import Collection

def insert_publications(collection: Collection, publications: list[str]):
  """
  Insert document into a ChromaDB collection.

  Args:
    collection (chromadb.collection): The collection to insert documents into
    publications (list[str]): The documents to insert

  Returns:
    None
  """

  next_id = collection.count()

  for publication in publications:
    chunked_publication = chunk_publication(publication)
    embeddings = embed_documents(chunked_publication)
    ids = list(range(next_id, next_id + len(chunked_publication)))
    ids = [f"document_{id}" for id in ids]
    collection.add(
        embeddings=embeddings,
        ids=ids,
        documents=chunked_publication
    )
    next_id += len(chunked_publication)

"""# Intelligent Retrieval"""

def search_reasearch_db(query, collection, embeddings, top_k=5):
  """Find the most relevant research chunk for the querry"""

  # Convert question to vector
  query_vector = embeddings.embed_query(query)

  # Search for similar contnet
  results = collection.query(
      query_embeddings = [query_vector],
      n_results=top_k,
      include=["documents", "metadatas", "distances"]
  )

  # Format results
  relevant_chunks = []
  for i, doc in enumerate(results["documents"][0]):
    relevant_chunks.append({
        "content": doc,
        "title": results["metadatas"][0][i]["title"],
        "similarity": 1 - results["distance"][0][i]  # Convert distance to similarity
    })

  return relevant_chunks

"""# Generating Research-Backed Answer"""

!pip install langchain-groq

from langchain_groq import ChatGroq
from langchain.prompts import PromptTemplate

def answer_research_question(query, collection, embeddings, llm):
  """Generate an answer based on retrieved research"""

  # get relevant research chunks
  relevant_chunks = search_research_db(query, collection, embeddings, top_k=3)

  # Build context from research
  context = "\n\n".join([
      f"From {chunk['title']}:\n{chunk['content']}"
      for chunk in relevant_chunks
  ])

  # Create research focussed prompt
  prompt_template = PromptTemplate(
      input_variable = ["context", "question"],
      template = """
      Based on the following research findings, answer the researcher's question:

      Research context:
      {context}

      Researcher's Question:
      {question}

      Answer: Provide a comprehensive answer based on the research findings above.
      """
  )

  # Generate answer
  prompt = prompt_template.format(context=context, question=query)
  response=llm.invoke(prompt)

  return response.content, relevant_chunks

# Initialize LLM and get answer
llm = ChatGroq(model="llama3-8b-8192")
answer, sources = answer_research_question(
    "What are the effective techniques for handling class imbaance?",
    collection,
    embeddings,
    llm
)

print("AI Answer:", answer)
print("\nBased on sources:")
for source in sources:
  print(f"- {source['title']}")

