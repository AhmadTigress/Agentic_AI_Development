# -*- coding: utf-8 -*-
"""week_2_lesson_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10pyoBoaKGxi5zQSDC8JWumA3n6pmRuxL

**Regular Prompting with Format Hints**
"""

from langchain.prompts import PromptTemplate

json_prompt = PromptTemplate(
    input_variables = ["product_description"],
    template="""
    Create a product listing based on on this description: {product_description}

    Return the information as a json object with the following keys:
    - name: product name
    - price: suggested price as number
    - features: array of key product features
    - category: product category

    Example format:
    {{
      "name": "Wireless EarBuds",
      "price": 79.99,
      "features": ["Noise cancellation", "Waterproof", "20hr battery life"],
      "category": "Electronics"
    }}
    """
)

"""**Native Structured Output with LangChain**"""

!pip install -q langchain
!pip install -q langchain-openai
!pip install -q langchain-core python-dotenv
print("Installed!")

# Mount google drive in new notebook
from google.colab import drive
drive.mount("/content/drive")

# Key from my saved file
with open("/content/drive/MyDrive/secret_key.txt", "r") as file:
  api_key = file.read().strip()

"""**CAUTION**

This code below is display `error` because `ChatOpenAI` requires billing plan before using it. I don't have any billing plan on `OpenAI` As such, i could not proceed.

If you have `access`, you can clone this repo and run it on your local machine or cloud.
Thanks
"""

from typing import List
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

load_dotenv()

class Product(BaseModel):
  name: str = Field("The name of the product.")
  price: float = Field("The product's price.")
  features: List[str] = Field("Product's features.")
  category: str = Field("Product category. One of [Beverages, Diary, Grocery]")

# Initialize the model
model = ChatOpenAI(model_name='gpt-4o', temperature=0, api_key=api_key)

# Define the prompt
prompt = ChatPromptTemplate.from_template(
    "Generate product information for: {description}"
)

# Chain prompt, model, and schema
chain = prompt | model.with_structured_output(Product)

# Run the chain
result  = chain.invoke({"description": "Two kilos of tomato"})
print(result.model_dump())

"""# Open Source alternative
Using Hugging Face

**CAUTION**

I decided to use open source which is free and does not require billing plans. This is a good option, but i encoutered another challenge.

It is either the free models available are too large to use in Google Colab enviroment(12GB RAM) i.e "Mistral-7B-Instruct-v0.1" is 14GB size
              OR
The small models ("gpt2-medium" , size 1.5GB) are unable or not suitable to output the required prompt
"""

!pip install -q langchain
!pip install -q -U langchain-huggingface
#!pip install -U langchain langchain-community
print("Installed!")

from typing import List
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain_huggingface import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch

# Output schema
class Product(BaseModel):
  name: str = Field(..., description="The name of the product.")
  price: float = Field(..., description="The product's price.")
  features: List[str] = Field(..., description="Product's features")
  category: str = Field(..., description="Product category. One of [Beverages, Diary, Grocery]")

# Load HF model pipeline
pipe  = pipeline("text-generation", model="gpt2-medium")

# Langchain model
llm = HuggingFacePipeline(pipeline=pipe)

# prompt and parser
parser = PydanticOutputParser(pydantic_object=Product)
prompt = ChatPromptTemplate.from_template(
    "Generate a realistic product in JSON format given the description below.\n"
    "The JSON must match this structure exactly: \n\n{format_instructions}\n\n"
    "Description: {description}"
).partial(format_instructions=parser.get_format_instructions())

# chain and run
chain = prompt | llm | parser
result = chain.invoke({"description": "Two kilos of tomato"})
print(result.model_dump())

