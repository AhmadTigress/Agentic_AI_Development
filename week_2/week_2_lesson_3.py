# -*- coding: utf-8 -*-
"""week_2_lesson_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uEzVcXI3GurRMCdfpIFvp15KJORtc57Z

# Building Intelligent Pipelines: A Guide to LLM Function Chaining
"""

!pip install -q langchain
!pip install -q -U langchain-huggingface
print("Installed!")

from langchain_core.prompts import PromptTemplate
#from langchain_openai import ChatOpenAI
from transformers import pipeline
from langchain_huggingface import HuggingFacePipeline
#from dotenv import load_dotenv

#load_dotenv()

pipe = pipeline(
    "text-generation",
    model="google/flan-t5-base"
)

# model
model = HuggingFacePipeline(pipeline=pipe)


# Create a prompt template with placeholders
template = PromptTemplate(
    input_variables=["country"],
    template="what is the capital of: {country}?"
)


for country in ['France', 'Spain', 'Germany']:
  formatted_prompt = template.format(country=country)
  print("Formatted prompt:", formatted_prompt)

  response = model.invoke(formatted_prompt)
  print("\nResponse:", response,'\n-------')

"""# Customer Support's template"""

from langchain_core.prompts import PromptTemplate

customer_support_template = PromptTemplate(
    input_variables = ["customer_name", "product_name", "issue_description", "previous_interactions", "tone"],
    template = """
    You are a customer support specialist for a {product_name}.

    Customer: {customer_name}
    Issue: {issue_description}
    Previous interactions: {previous_interactions}

    Respond to the customer in a {tone} tone. If you don't have any information to resolve their issue, ask clarifying questions.
    Always priotize customer's satisfaction and accurate information.
    """
)

response = customer_support_template.invoke({
    "customer_name": "Alex Smith",
    "product_name": "SmartHome Hub",
    "issue_description": "Device won't connect to WiFi after power outage",
    "previous_interactions": "Customer has already tried to reset the device twice",
    "tone": "empathetic but technical"
})

"""# Function chaining

**Questions**
"""

# Create prompt template
prompt= PromptTemplate(
    input_variables = ["topic"],
    template = "Generate 3 questions about {topic}"
)

pipe = pipeline(
    "text2text-generation",
    model="google/flan-t5-base"
)
# llms model
llm = HuggingFacePipeline(pipeline=pipe)


# Create a chain by binding the prompt to the model
question_chain = prompt | llm

# Invoke the chain
questions = question_chain.invoke({"topic": "artificial intelligence"})

"""**Questions and Answers**

# CAUTION

The `code` below is okay, but the model is not suitable for such task. A `larger model` is needed for efficient execution.
My Google colab limitation is `12GB RAM`. `Clone` and run this in a better enviroment.
Thanks.
"""

from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda

# First chain generates questions
question_prompt = PromptTemplate(
    input_variables=["topic"],
    template="Generate 3 questions about {topic}"
)

# Second chain generates answers based on questions
answer_prompt = PromptTemplate(
    input_variables=["questions"],
    template="Answer the following questions:\n{questions}\n Your response should contain question and the answer to it."
)

pipe = pipeline(
    "text2text-generation",
    model="google/flan-t5-large",
    device=-1
)

# llm model
llm = HuggingFacePipeline(pipeline=pipe)

# Output parser to convert model output to a string
output_parser = StrOutputParser()

# Build the question generation chain
question_chain = question_prompt | llm | output_parser

# Build the answer generation chain
answer_chain = answer_prompt | llm | output_parser

# Define a function to create the combined input for the answer chain
def create_answer_input(output):
  return {"questions": output}

# Chain everything together
qa_chain = question_chain | RunnableLambda(create_answer_input) | answer_chain

# Run the chain
result = qa_chain.invoke({"topic": "artificial intelligence"})
print(result)

