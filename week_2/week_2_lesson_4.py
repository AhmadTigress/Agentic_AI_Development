# -*- coding: utf-8 -*-
"""week_2_lesson_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lfIIUY2bxTtSQrOfEd8DMtFrPf6HjFbh

# Choose an Embedding Model
"""

!pip install langchain_community

"""# NOTE

Do not forget to include your HF token.
"""

from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

"""#Prepare the Documents"""

from langchain_core.documents import Document

texts = [
    "Vector databases enables semantic search by storing embeddings",
    "RAG systems combine retrieval with language model generation",
    "Embeddings caputre semantic meaning in numerical form"
]

metadatas = [
    {"topic": "database", "type": "technical"},
    {"topic": "AI", "type": "technical"},
    {"topic": "ML", "type": "technical"}
]

documents = [
    Document(page_content=text, metadata=metadatas[i])
    for i, text in enumerate(texts)
]

"""# Create the Vector Store"""

!pip install chromadb

from langchain_community.vectorstores import Chroma

vectorstore = Chroma.from_documents(documents, embeddings)

"""# Search by Meaning"""

results = vectorstore.similarity_search_with_score("What is a RAG system?", k=2)

for doc, score in results:
  print(f"Score: {score:.3f}")
  print(f"text: {doc.page_content}")
  print(f"Metadata: {doc.metadata}")
  print("---")

"""# Split Text"""

from langchain.text_splitter import RecursiveCharacterTextSplitter

# Define long text
long_text = """
Artificial Intelligence (AI) is transforming the healthcare industry.
From diagnostic imaging to predictive analytics, AI helps providers make faster and more accurate decisions.
However, with great power comes great responsibility. Ethical concerns such as data privacy, bias, and transparency must be addressed to ensure trustworthy AI systems.
The future of healthcare depends not just on technological advancements, but also on human-centered design and careful regulation.
"""

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

chunks = splitter.split_text(long_text)

"""# Document Processing Pipeline"""

def process_document_file(file_path):
  # Read the document
  with open(file_path, 'r', encoding='utf-8') as f:
    text = f.read()

  # Split intelligently
  splitter = RecursiveCharacterTextSplitter(
      chunk_size=500,
      chunk_overlap=50
  )
  chunks = splitter.split_text(text)

  # Create documents with metadata
  documents = [
      Document(
          page_content=chunk,
          metadata={"source": file_path, "chunk_id": i}
      )
      for i, chunk in enumerate(chunks)
  ]

  # Create searchable vector store
  embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
  vectorstore = Chroma.from_documents(documents, embeddings)

  return vectorstore

