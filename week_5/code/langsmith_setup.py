# -*- coding: utf-8 -*-
"""LangSmith_Setup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xV7oznfKFQO_yuZ5wI5HiTjkS_Cj80UO

# Using LangChain or LangGraph
"""

# Install dependencies

#!pip install -U langchain langchain-openai

# Set enviroment variables
import os

os.environ["LANGSMITH_TRACING"]="true"
os.environ["LANGSMITH_ENDPOINT"]="https://api.smith.langchain.com"
os.environ["LANGSMITH_API_KEY"]="<your-api-key>"
os.environ["LANGSMITH_PROJECT"]="your-project-name"
os.environ["OPEN_API_KEY"]="<your-openai-api-key>"

# Run code

from langchain_openai import ChatOpenAI

llm = ChatOpenAI()
llm.invoke("Hello, world!")

"""# Using Custom Code (no LangChain)"""

# Install
#!pip install -U langsmith

# Set enviroment variables
import os

os.environ["LANGSMITH_TRACING"]="true"
os.environ["LANGSMITH_ENDPOINT"]="https://api.smith.langchain.com"
os.environ["LANGSMITH_API_KEY"]="<your-api-key>"
os.environ["LANGSMITH_PROJECT"]="your-project-name"
os.environ["OPEN_API_KEY"]="<your-openai-api-key>"

## Wrap your OpenAI client and trace your function




import openai
from langsmith.wrappers import wrap_openai
from langsmith import traceable

# Autotrace OpenAI calls
client = wrap_openai(openai.Client())

@traceable
def pipeline(user_input: str):
  result = client.chat.completions.create(
      messages=[{"role": "user", "content": user_input}],
      model="gpt-3.5-turbo"
  )
  return result.choices[0].message.content

pipeline("Hello, world!")

